{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c5991",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'TypeAliasType' could not be imported from 'c:\\Users\\yashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typing_extensions.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Cell 1 - Dataset Acquisition with Enhanced Metadata\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Download RAVDESS dataset\n",
    "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# RAVDESS emotion mapping\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "emotion_intensity = {\n",
    "    '01': 1,  # Neutral\n",
    "    '02': 2,  # Calm\n",
    "    '03': 2,  # Happy\n",
    "    '04': 2,  # Sad\n",
    "    '05': 2,  # Angry\n",
    "    '06': 2,  # Fearful\n",
    "    '07': 2,  # Disgust\n",
    "    '08': 2   # Surprised\n",
    "}\n",
    "\n",
    "# Find all .wav files\n",
    "search_patterns = [\n",
    "    os.path.join(path, \"Actor_*\", \"*.wav\"),\n",
    "    os.path.join(path, \"audio_speech_actors_01-24\", \"Actor_*\", \"*.wav\"),\n",
    "    os.path.join(path, \"**\", \"*.wav\")\n",
    "]\n",
    "\n",
    "wav_files = []\n",
    "for pattern in search_patterns:\n",
    "    matches = glob.glob(pattern, recursive=True)\n",
    "    wav_files.extend(matches)\n",
    "\n",
    "wav_files = sorted(list(set(wav_files)))\n",
    "\n",
    "# Parse filenames and extract comprehensive metadata\n",
    "metadata = {\n",
    "    'filepath': [],\n",
    "    'filename': [],\n",
    "    'actor_id': [],\n",
    "    'emotion_id': [],\n",
    "    'emotion_label': [],\n",
    "    'intensity': [],\n",
    "    'statement': [],\n",
    "    'repetition': [],\n",
    "    'gender': []\n",
    "}\n",
    "\n",
    "for filepath in wav_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    parts = filename.split('-')\n",
    "\n",
    "    if len(parts) >= 7:\n",
    "        emotion_id = parts[2]\n",
    "        if emotion_id in emotion_map:\n",
    "            metadata['filepath'].append(filepath)\n",
    "            metadata['filename'].append(filename)\n",
    "            metadata['actor_id'].append(parts[6].split('.')[0])\n",
    "            metadata['emotion_id'].append(emotion_id)\n",
    "            metadata['emotion_label'].append(emotion_map[emotion_id])\n",
    "            metadata['intensity'].append(emotion_intensity.get(parts[3], 1))\n",
    "            metadata['statement'].append(parts[4])\n",
    "            metadata['repetition'].append(parts[5])\n",
    "            metadata['gender'].append('female' if int(parts[6].split('.')[0]) % 2 == 0 else 'male')\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "print(f\"Total audio files detected: {len(df_metadata)}\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Unique actors: {df_metadata['actor_id'].nunique()}\")\n",
    "print(f\"Unique emotions: {df_metadata['emotion_label'].nunique()}\")\n",
    "print(f\"Gender distribution:\")\n",
    "print(df_metadata['gender'].value_counts())\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "emotion_counts = df_metadata['emotion_label'].value_counts()\n",
    "print(emotion_counts)\n",
    "\n",
    "# Plot dataset distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Emotion distribution\n",
    "ax1 = axes[0, 0]\n",
    "emotion_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Distribution of Emotion Classes', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Emotion', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gender distribution\n",
    "ax2 = axes[0, 1]\n",
    "gender_counts = df_metadata['gender'].value_counts()\n",
    "gender_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "ax2.set_title('Gender Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# Actor-wise distribution\n",
    "ax3 = axes[1, 0]\n",
    "actor_counts = df_metadata['actor_id'].value_counts().sort_index()\n",
    "actor_counts.plot(kind='bar', ax=ax3, color='lightgreen', width=0.8)\n",
    "ax3.set_title('Samples per Actor', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Actor ID', fontsize=12)\n",
    "ax3.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Intensity distribution\n",
    "ax4 = axes[1, 1]\n",
    "intensity_counts = df_metadata['intensity'].value_counts().sort_index()\n",
    "intensity_counts.plot(kind='bar', ax=ax4, color='gold')\n",
    "ax4.set_title('Intensity Level Distribution', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Intensity Level', fontsize=12)\n",
    "ax4.set_ylabel('Count', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for model\n",
    "filepaths = df_metadata['filepath'].tolist()\n",
    "labels = df_metadata['emotion_label'].tolist()\n",
    "actor_ids = df_metadata['actor_id'].tolist()\n",
    "genders = df_metadata['gender'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY FOR PUBLICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Samples: {len(filepaths)}\")\n",
    "print(f\"Number of Actors: {len(set(actor_ids))}\")\n",
    "print(f\"Number of Emotion Classes: {len(set(labels))}\")\n",
    "print(f\"Classes: {sorted(set(labels))}\")\n",
    "print(f\"Average samples per class: {len(filepaths)/len(set(labels)):.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aff06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 10:36:05.462169: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-09 10:36:05.511299: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-09 10:36:05.858150: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-09 10:36:05.860173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-09 10:36:07.314892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting combined features (Specific + Traditional)...\n",
      "Processed 0/2880 files...\n",
      "Processed 50/2880 files...\n",
      "Processed 100/2880 files...\n",
      "Processed 150/2880 files...\n",
      "Processed 200/2880 files...\n",
      "Processed 250/2880 files...\n",
      "Processed 300/2880 files...\n",
      "Processed 350/2880 files...\n",
      "Processed 400/2880 files...\n",
      "Processed 450/2880 files...\n",
      "Processed 500/2880 files...\n",
      "Processed 550/2880 files...\n",
      "Processed 600/2880 files...\n",
      "Processed 650/2880 files...\n",
      "Processed 700/2880 files...\n",
      "Processed 750/2880 files...\n",
      "Processed 800/2880 files...\n",
      "Processed 850/2880 files...\n",
      "Processed 900/2880 files...\n",
      "Processed 950/2880 files...\n",
      "Processed 1000/2880 files...\n",
      "Processed 1050/2880 files...\n",
      "Processed 1100/2880 files...\n",
      "Processed 1150/2880 files...\n",
      "Processed 1200/2880 files...\n",
      "Processed 1250/2880 files...\n",
      "Processed 1300/2880 files...\n",
      "Processed 1350/2880 files...\n",
      "Processed 1400/2880 files...\n",
      "Processed 1450/2880 files...\n",
      "Processed 1500/2880 files...\n",
      "Processed 1550/2880 files...\n",
      "Processed 1600/2880 files...\n",
      "Processed 1650/2880 files...\n",
      "Processed 1700/2880 files...\n",
      "Processed 1750/2880 files...\n",
      "Processed 1800/2880 files...\n",
      "Processed 1850/2880 files...\n",
      "Processed 1900/2880 files...\n",
      "Processed 1950/2880 files...\n",
      "Processed 2000/2880 files...\n",
      "Processed 2050/2880 files...\n",
      "Processed 2100/2880 files...\n",
      "Processed 2150/2880 files...\n",
      "Processed 2200/2880 files...\n",
      "Processed 2250/2880 files...\n",
      "Processed 2300/2880 files...\n",
      "Processed 2350/2880 files...\n",
      "Processed 2400/2880 files...\n",
      "Processed 2450/2880 files...\n",
      "Processed 2500/2880 files...\n",
      "Processed 2550/2880 files...\n",
      "Processed 2600/2880 files...\n",
      "Processed 2650/2880 files...\n",
      "Processed 2700/2880 files...\n",
      "Processed 2750/2880 files...\n",
      "Processed 2800/2880 files...\n",
      "Processed 2850/2880 files...\n",
      "\n",
      "Combined feature extraction complete:\n",
      "Data shape: (2880, 130, 203)\n",
      "Labels shape: (2880, 8)\n",
      "Number of features per timestep: 203\n",
      "\n",
      "Detailed Feature Breakdown:\n",
      "==================================================\n",
      "FRAME-LEVEL FEATURES (129 features):\n",
      "--------------------------------------------------\n",
      "Traditional Features:\n",
      "  • MFCCs: 40 features\n",
      "  • Delta MFCCs: 20 features\n",
      "  • Delta2 MFCCs: 20 features\n",
      "  • Log-Mel Spectrogram: 40 features\n",
      "\n",
      "Specific Features:\n",
      "  • Spectral Contrast: 7 features (6 bands + mean)\n",
      "  • Zero Crossing Rate: 1 feature\n",
      "  • Spectral Flux: 1 feature\n",
      "  Frame-level Total: 129 features\n",
      "\n",
      "==================================================\n",
      "UTTERANCE-LEVEL FEATURES (74 features):\n",
      "--------------------------------------------------\n",
      "Specific Features:\n",
      "  • Spectral Entropy: 1 feature\n",
      "  • Renyi Entropy: 1 feature\n",
      "  • TEO features: 3 features\n",
      "  • HPSS features: 3 features\n",
      "  • Enhanced ZCR: 3 features\n",
      "  • Spectral Contrast stats: 14 features\n",
      "  • Spectral Flux stats: 3 features\n",
      "  • VMD-like features: 4 features\n",
      "  • RMS Energy & Entropy: 3 features\n",
      "  • Total Energy: 1 feature\n",
      "  • Energy Entropy: 1 feature\n",
      "  • Spectral Energy Entropy: 1 feature\n",
      "  • Spectral Energy in 4 Subbands: 4 features\n",
      "  • Permutation Entropy: 1 feature\n",
      "  • Skewness & Kurtosis: 2 features\n",
      "  • Spectral Roll-off stats: 2 features\n",
      "  • Pitch features: 3 features\n",
      "  • Chroma features stats: 24 features\n",
      "  Utterance-level Total: 74 features\n",
      "\n",
      "==================================================\n",
      "GRAND TOTAL: 203 FEATURES\n",
      "  • Frame-level: 129 features\n",
      "  • Utterance-level: 74 features\n",
      "  • Combined: 203 features per timestep\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "============================================================\n",
      "STEP 1: Training Autoencoder on all data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 10:48:14.406449: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder architecture summary:\n",
      "Total parameters: 1,056,219\n",
      "\n",
      "Training autoencoder with early stopping...\n",
      "Epoch 1/50\n",
      "108/108 [==============================] - 12s 65ms/step - loss: 0.6800 - mae: 0.5572 - val_loss: 0.6058 - val_mae: 0.5284 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.5352 - mae: 0.4878 - val_loss: 0.5169 - val_mae: 0.4764 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.4976 - mae: 0.4681 - val_loss: 0.4899 - val_mae: 0.4666 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.4719 - mae: 0.4538 - val_loss: 0.4701 - val_mae: 0.4560 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.4555 - mae: 0.4455 - val_loss: 0.4428 - val_mae: 0.4368 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.4412 - mae: 0.4380 - val_loss: 0.4369 - val_mae: 0.4355 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.4272 - mae: 0.4295 - val_loss: 0.4159 - val_mae: 0.4228 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.4180 - mae: 0.4256 - val_loss: 0.4040 - val_mae: 0.4148 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.4107 - mae: 0.4221 - val_loss: 0.3964 - val_mae: 0.4092 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.4002 - mae: 0.4152 - val_loss: 0.3921 - val_mae: 0.4111 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3943 - mae: 0.4128 - val_loss: 0.3832 - val_mae: 0.4033 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3879 - mae: 0.4091 - val_loss: 0.3753 - val_mae: 0.3987 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3829 - mae: 0.4066 - val_loss: 0.3671 - val_mae: 0.3938 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3766 - mae: 0.4026 - val_loss: 0.3639 - val_mae: 0.3906 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3721 - mae: 0.4007 - val_loss: 0.3568 - val_mae: 0.3854 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3660 - mae: 0.3970 - val_loss: 0.3522 - val_mae: 0.3826 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3612 - mae: 0.3936 - val_loss: 0.3460 - val_mae: 0.3792 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3583 - mae: 0.3923 - val_loss: 0.3444 - val_mae: 0.3791 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3535 - mae: 0.3890 - val_loss: 0.3367 - val_mae: 0.3728 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3535 - mae: 0.3904 - val_loss: 0.3398 - val_mae: 0.3775 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3484 - mae: 0.3870 - val_loss: 0.3322 - val_mae: 0.3720 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3470 - mae: 0.3871 - val_loss: 0.3306 - val_mae: 0.3697 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3415 - mae: 0.3827 - val_loss: 0.3255 - val_mae: 0.3664 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3394 - mae: 0.3817 - val_loss: 0.3230 - val_mae: 0.3662 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3372 - mae: 0.3808 - val_loss: 0.3238 - val_mae: 0.3662 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3355 - mae: 0.3795 - val_loss: 0.3183 - val_mae: 0.3614 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3334 - mae: 0.3789 - val_loss: 0.3148 - val_mae: 0.3592 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3301 - mae: 0.3762 - val_loss: 0.3130 - val_mae: 0.3581 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3297 - mae: 0.3767 - val_loss: 0.3122 - val_mae: 0.3587 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3275 - mae: 0.3751 - val_loss: 0.3077 - val_mae: 0.3535 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3236 - mae: 0.3713 - val_loss: 0.3084 - val_mae: 0.3562 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3227 - mae: 0.3714 - val_loss: 0.3100 - val_mae: 0.3605 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3231 - mae: 0.3731 - val_loss: 0.3031 - val_mae: 0.3520 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3200 - mae: 0.3704 - val_loss: 0.3023 - val_mae: 0.3514 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3197 - mae: 0.3712 - val_loss: 0.3024 - val_mae: 0.3533 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3166 - mae: 0.3681 - val_loss: 0.2986 - val_mae: 0.3494 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3155 - mae: 0.3679 - val_loss: 0.2965 - val_mae: 0.3472 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3151 - mae: 0.3685 - val_loss: 0.2961 - val_mae: 0.3480 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3127 - mae: 0.3665 - val_loss: 0.2954 - val_mae: 0.3480 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3120 - mae: 0.3661 - val_loss: 0.2984 - val_mae: 0.3518 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3093 - mae: 0.3636 - val_loss: 0.2920 - val_mae: 0.3463 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3099 - mae: 0.3657 - val_loss: 0.2888 - val_mae: 0.3421 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3089 - mae: 0.3642 - val_loss: 0.2911 - val_mae: 0.3461 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3080 - mae: 0.3640 - val_loss: 0.2924 - val_mae: 0.3497 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3058 - mae: 0.3624 - val_loss: 0.2862 - val_mae: 0.3407 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.3056 - mae: 0.3627 - val_loss: 0.2862 - val_mae: 0.3421 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3028 - mae: 0.3607 - val_loss: 0.2831 - val_mae: 0.3384 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3054 - mae: 0.3642 - val_loss: 0.2822 - val_mae: 0.3384 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.3014 - mae: 0.3602 - val_loss: 0.2803 - val_mae: 0.3368 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "108/108 [==============================] - 6s 59ms/step - loss: 0.3013 - mae: 0.3604 - val_loss: 0.2815 - val_mae: 0.3402 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder final loss: 0.3013\n",
      "Autoencoder final val loss: 0.2815\n",
      "\n",
      "============================================================\n",
      "STEP 2: 5-Fold Cross Validation for Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "Classifier parameters: 2,766,713\n",
      "\n",
      "Phase 1: Training classifier (encoder frozen)...\n",
      "Epoch 1/40\n",
      "96/96 [==============================] - 15s 74ms/step - loss: 2.9236 - accuracy: 0.1719 - val_loss: 2.3243 - val_accuracy: 0.1302 - lr: 5.0000e-04\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 2.3752 - accuracy: 0.2678 - val_loss: 2.0532 - val_accuracy: 0.3368 - lr: 5.0000e-04\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 2.0963 - accuracy: 0.3507 - val_loss: 1.7219 - val_accuracy: 0.4983 - lr: 5.0000e-04\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.9077 - accuracy: 0.4110 - val_loss: 1.6077 - val_accuracy: 0.5069 - lr: 5.0000e-04\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.7404 - accuracy: 0.4583 - val_loss: 1.3701 - val_accuracy: 0.5972 - lr: 5.0000e-04\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.6166 - accuracy: 0.5152 - val_loss: 1.3233 - val_accuracy: 0.5990 - lr: 5.0000e-04\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.4804 - accuracy: 0.5577 - val_loss: 1.3308 - val_accuracy: 0.6319 - lr: 5.0000e-04\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.4034 - accuracy: 0.5855 - val_loss: 1.0687 - val_accuracy: 0.7170 - lr: 5.0000e-04\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.3050 - accuracy: 0.6233 - val_loss: 0.9819 - val_accuracy: 0.7604 - lr: 5.0000e-04\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.1521 - accuracy: 0.6819 - val_loss: 0.8956 - val_accuracy: 0.7622 - lr: 5.0000e-04\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.1388 - accuracy: 0.6814 - val_loss: 0.9203 - val_accuracy: 0.7517 - lr: 5.0000e-04\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.0277 - accuracy: 0.7222 - val_loss: 0.8909 - val_accuracy: 0.8038 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.9835 - accuracy: 0.7465 - val_loss: 1.1047 - val_accuracy: 0.7188 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.9912 - accuracy: 0.7491 - val_loss: 0.7096 - val_accuracy: 0.8472 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.8674 - accuracy: 0.7947 - val_loss: 0.8310 - val_accuracy: 0.7986 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 5s 50ms/step - loss: 0.8002 - accuracy: 0.8121 - val_loss: 0.6431 - val_accuracy: 0.8767 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.7609 - accuracy: 0.8320 - val_loss: 0.6251 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.6531 - accuracy: 0.8633 - val_loss: 0.7068 - val_accuracy: 0.8507 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.6918 - accuracy: 0.8529 - val_loss: 0.5966 - val_accuracy: 0.8767 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.6569 - accuracy: 0.8542 - val_loss: 0.5995 - val_accuracy: 0.8976 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.6217 - accuracy: 0.8750 - val_loss: 0.6102 - val_accuracy: 0.9028 - lr: 5.0000e-04\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.6175 - accuracy: 0.8707 - val_loss: 0.6484 - val_accuracy: 0.8837 - lr: 5.0000e-04\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5565 - accuracy: 0.8906 - val_loss: 0.5504 - val_accuracy: 0.8924 - lr: 5.0000e-04\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5394 - accuracy: 0.8937 - val_loss: 0.5530 - val_accuracy: 0.9149 - lr: 5.0000e-04\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4767 - accuracy: 0.9171 - val_loss: 0.5238 - val_accuracy: 0.9132 - lr: 5.0000e-04\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4533 - accuracy: 0.9223 - val_loss: 0.4998 - val_accuracy: 0.9149 - lr: 5.0000e-04\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.4469 - accuracy: 0.9245 - val_loss: 0.6332 - val_accuracy: 0.8646 - lr: 5.0000e-04\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5044 - accuracy: 0.9036 - val_loss: 0.6040 - val_accuracy: 0.8941 - lr: 5.0000e-04\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.4898 - accuracy: 0.9076 - val_loss: 0.5852 - val_accuracy: 0.8872 - lr: 5.0000e-04\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.4645 - accuracy: 0.9119 - val_loss: 0.4883 - val_accuracy: 0.9097 - lr: 5.0000e-04\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4237 - accuracy: 0.9253 - val_loss: 0.5971 - val_accuracy: 0.8872 - lr: 5.0000e-04\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.9349Restoring model weights from the end of the best epoch: 24.\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.3928 - accuracy: 0.9349 - val_loss: 0.5557 - val_accuracy: 0.9028 - lr: 5.0000e-04\n",
      "Epoch 32: early stopping\n",
      "\n",
      "Phase 2: Fine-tuning autoencoder blocks...\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 15s 74ms/step - loss: 0.4490 - accuracy: 0.9297 - val_loss: 0.4693 - val_accuracy: 0.9201\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.3762 - accuracy: 0.9475 - val_loss: 0.4375 - val_accuracy: 0.9236\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.3540 - accuracy: 0.9575 - val_loss: 0.4384 - val_accuracy: 0.9306\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.3316 - accuracy: 0.9583 - val_loss: 0.4410 - val_accuracy: 0.9410\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.3042 - accuracy: 0.9696 - val_loss: 0.4318 - val_accuracy: 0.9444\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.3138 - accuracy: 0.9648 - val_loss: 0.4299 - val_accuracy: 0.9375\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.2984 - accuracy: 0.9670 - val_loss: 0.4055 - val_accuracy: 0.9497\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.2759 - accuracy: 0.9766 - val_loss: 0.3904 - val_accuracy: 0.9392\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.2851 - accuracy: 0.9692 - val_loss: 0.3913 - val_accuracy: 0.9444\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.2590 - accuracy: 0.9792 - val_loss: 0.3828 - val_accuracy: 0.9444\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.2667 - accuracy: 0.9748 - val_loss: 0.3796 - val_accuracy: 0.9427\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.2462 - accuracy: 0.9831 - val_loss: 0.3785 - val_accuracy: 0.9444\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9761Restoring model weights from the end of the best epoch: 7.\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.2468 - accuracy: 0.9761 - val_loss: 0.4223 - val_accuracy: 0.9375\n",
      "Epoch 13: early stopping\n",
      "\n",
      "Fold 1 Results:\n",
      "  Accuracy:  0.9497\n",
      "  Precision: 0.9513\n",
      "  Recall:    0.9465\n",
      "  F1-Score:  0.9480\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "Classifier parameters: 2,766,713\n",
      "\n",
      "Phase 1: Training classifier (encoder frozen)...\n",
      "Epoch 1/40\n",
      "96/96 [==============================] - 15s 77ms/step - loss: 2.8742 - accuracy: 0.1827 - val_loss: 2.3194 - val_accuracy: 0.2986 - lr: 5.0000e-04\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 2.3673 - accuracy: 0.2669 - val_loss: 2.0595 - val_accuracy: 0.4271 - lr: 5.0000e-04\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 2.1156 - accuracy: 0.3216 - val_loss: 1.7721 - val_accuracy: 0.4792 - lr: 5.0000e-04\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 1.9232 - accuracy: 0.3924 - val_loss: 1.6618 - val_accuracy: 0.4722 - lr: 5.0000e-04\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 1.7548 - accuracy: 0.4583 - val_loss: 1.3063 - val_accuracy: 0.6302 - lr: 5.0000e-04\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.6507 - accuracy: 0.4944 - val_loss: 1.2434 - val_accuracy: 0.6302 - lr: 5.0000e-04\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.4544 - accuracy: 0.5577 - val_loss: 1.1466 - val_accuracy: 0.6580 - lr: 5.0000e-04\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 1.3453 - accuracy: 0.6120 - val_loss: 1.0675 - val_accuracy: 0.7188 - lr: 5.0000e-04\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.2590 - accuracy: 0.6506 - val_loss: 0.9697 - val_accuracy: 0.7222 - lr: 5.0000e-04\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.1732 - accuracy: 0.6667 - val_loss: 0.8877 - val_accuracy: 0.7656 - lr: 5.0000e-04\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.0645 - accuracy: 0.7183 - val_loss: 0.8282 - val_accuracy: 0.7951 - lr: 5.0000e-04\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.9907 - accuracy: 0.7391 - val_loss: 0.8703 - val_accuracy: 0.8090 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.9958 - accuracy: 0.7470 - val_loss: 0.8002 - val_accuracy: 0.8021 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.8735 - accuracy: 0.7930 - val_loss: 0.6369 - val_accuracy: 0.8681 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.8092 - accuracy: 0.8043 - val_loss: 0.6998 - val_accuracy: 0.8524 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.7881 - accuracy: 0.8173 - val_loss: 0.6168 - val_accuracy: 0.8663 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.6965 - accuracy: 0.8563 - val_loss: 0.6060 - val_accuracy: 0.8681 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.6772 - accuracy: 0.8550 - val_loss: 0.6749 - val_accuracy: 0.8733 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.6316 - accuracy: 0.8724 - val_loss: 0.6008 - val_accuracy: 0.8993 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.6681 - accuracy: 0.8633 - val_loss: 0.5392 - val_accuracy: 0.9028 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.5953 - accuracy: 0.8841 - val_loss: 0.5201 - val_accuracy: 0.9010 - lr: 5.0000e-04\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5295 - accuracy: 0.9062 - val_loss: 0.5453 - val_accuracy: 0.9010 - lr: 5.0000e-04\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5479 - accuracy: 0.8919 - val_loss: 0.4949 - val_accuracy: 0.9132 - lr: 5.0000e-04\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5539 - accuracy: 0.8963 - val_loss: 0.6217 - val_accuracy: 0.8872 - lr: 5.0000e-04\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5202 - accuracy: 0.9067 - val_loss: 0.4999 - val_accuracy: 0.9167 - lr: 5.0000e-04\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.5008 - accuracy: 0.9067 - val_loss: 0.4841 - val_accuracy: 0.9201 - lr: 5.0000e-04\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.4426 - accuracy: 0.9275 - val_loss: 0.4821 - val_accuracy: 0.9132 - lr: 5.0000e-04\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4620 - accuracy: 0.9132 - val_loss: 0.5529 - val_accuracy: 0.8924 - lr: 5.0000e-04\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.4956 - accuracy: 0.9049 - val_loss: 0.4920 - val_accuracy: 0.9115 - lr: 5.0000e-04\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4439 - accuracy: 0.9171 - val_loss: 0.4533 - val_accuracy: 0.9201 - lr: 5.0000e-04\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.4488 - accuracy: 0.9162 - val_loss: 0.5016 - val_accuracy: 0.9115 - lr: 5.0000e-04\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.3985 - accuracy: 0.9353 - val_loss: 0.4365 - val_accuracy: 0.9080 - lr: 5.0000e-04\n",
      "Epoch 33/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.3694 - accuracy: 0.9414 - val_loss: 0.5292 - val_accuracy: 0.9010 - lr: 5.0000e-04\n",
      "Epoch 34/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.3837 - accuracy: 0.9336 - val_loss: 0.3702 - val_accuracy: 0.9410 - lr: 5.0000e-04\n",
      "Epoch 35/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.3194 - accuracy: 0.9523 - val_loss: 0.4445 - val_accuracy: 0.9410 - lr: 5.0000e-04\n",
      "Epoch 36/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.3047 - accuracy: 0.9549 - val_loss: 0.4870 - val_accuracy: 0.9288 - lr: 5.0000e-04\n",
      "Epoch 37/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.3510 - accuracy: 0.9431 - val_loss: 0.4347 - val_accuracy: 0.9306 - lr: 5.0000e-04\n",
      "Epoch 38/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.3322 - accuracy: 0.9431 - val_loss: 0.3403 - val_accuracy: 0.9358 - lr: 5.0000e-04\n",
      "Epoch 39/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.2934 - accuracy: 0.9557 - val_loss: 0.4669 - val_accuracy: 0.9306 - lr: 5.0000e-04\n",
      "Epoch 40/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.3244 - accuracy: 0.9514 - val_loss: 0.5666 - val_accuracy: 0.9010 - lr: 5.0000e-04\n",
      "\n",
      "Phase 2: Fine-tuning autoencoder blocks...\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 15s 74ms/step - loss: 0.2511 - accuracy: 0.9674 - val_loss: 0.4346 - val_accuracy: 0.9306\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 6s 61ms/step - loss: 0.2151 - accuracy: 0.9783 - val_loss: 0.4309 - val_accuracy: 0.9375\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1908 - accuracy: 0.9852 - val_loss: 0.4377 - val_accuracy: 0.9340\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1918 - accuracy: 0.9822 - val_loss: 0.4120 - val_accuracy: 0.9410\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1722 - accuracy: 0.9896 - val_loss: 0.4257 - val_accuracy: 0.9375\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1756 - accuracy: 0.9878 - val_loss: 0.4078 - val_accuracy: 0.9410\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1596 - accuracy: 0.9900 - val_loss: 0.4166 - val_accuracy: 0.9444\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1560 - accuracy: 0.9931 - val_loss: 0.4094 - val_accuracy: 0.9375\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1542 - accuracy: 0.9913 - val_loss: 0.4370 - val_accuracy: 0.9375\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1500 - accuracy: 0.9913 - val_loss: 0.4186 - val_accuracy: 0.9410\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1514 - accuracy: 0.9909 - val_loss: 0.4397 - val_accuracy: 0.9375\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.1500 - accuracy: 0.9900 - val_loss: 0.4464 - val_accuracy: 0.9375\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1436 - accuracy: 0.9935 - val_loss: 0.4357 - val_accuracy: 0.9479\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1372 - accuracy: 0.9952 - val_loss: 0.4416 - val_accuracy: 0.9340\n",
      "Epoch 15/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1327 - accuracy: 0.9965 - val_loss: 0.4376 - val_accuracy: 0.9410\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1410 - accuracy: 0.9935 - val_loss: 0.4511 - val_accuracy: 0.9375\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1347 - accuracy: 0.9926 - val_loss: 0.4671 - val_accuracy: 0.9340\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.1287 - accuracy: 0.9935 - val_loss: 0.4424 - val_accuracy: 0.9340\n",
      "Epoch 19/20\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9957Restoring model weights from the end of the best epoch: 13.\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.1265 - accuracy: 0.9957 - val_loss: 0.4413 - val_accuracy: 0.9375\n",
      "Epoch 19: early stopping\n",
      "\n",
      "Fold 2 Results:\n",
      "  Accuracy:  0.9479\n",
      "  Precision: 0.9452\n",
      "  Recall:    0.9511\n",
      "  F1-Score:  0.9470\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "Classifier parameters: 2,766,713\n",
      "\n",
      "Phase 1: Training classifier (encoder frozen)...\n",
      "Epoch 1/40\n",
      "96/96 [==============================] - 15s 73ms/step - loss: 2.8499 - accuracy: 0.1753 - val_loss: 2.2893 - val_accuracy: 0.1753 - lr: 5.0000e-04\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 2.4196 - accuracy: 0.2643 - val_loss: 2.1256 - val_accuracy: 0.3507 - lr: 5.0000e-04\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 2.1232 - accuracy: 0.3303 - val_loss: 1.8027 - val_accuracy: 0.4670 - lr: 5.0000e-04\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.9448 - accuracy: 0.3889 - val_loss: 1.6349 - val_accuracy: 0.5139 - lr: 5.0000e-04\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.8150 - accuracy: 0.4206 - val_loss: 1.5420 - val_accuracy: 0.5278 - lr: 5.0000e-04\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 1.7029 - accuracy: 0.4705 - val_loss: 1.4190 - val_accuracy: 0.5677 - lr: 5.0000e-04\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.5448 - accuracy: 0.5382 - val_loss: 1.3878 - val_accuracy: 0.5833 - lr: 5.0000e-04\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 1.4292 - accuracy: 0.5716 - val_loss: 1.2569 - val_accuracy: 0.6267 - lr: 5.0000e-04\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.3299 - accuracy: 0.6137 - val_loss: 1.1593 - val_accuracy: 0.6701 - lr: 5.0000e-04\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.2313 - accuracy: 0.6484 - val_loss: 1.0600 - val_accuracy: 0.7066 - lr: 5.0000e-04\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 1.1550 - accuracy: 0.6827 - val_loss: 0.9842 - val_accuracy: 0.6892 - lr: 5.0000e-04\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 1.1163 - accuracy: 0.6897 - val_loss: 1.0384 - val_accuracy: 0.7170 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 1.0176 - accuracy: 0.7318 - val_loss: 0.9472 - val_accuracy: 0.7604 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.9213 - accuracy: 0.7778 - val_loss: 0.8122 - val_accuracy: 0.7760 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.8942 - accuracy: 0.7791 - val_loss: 1.0296 - val_accuracy: 0.7656 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.8317 - accuracy: 0.7982 - val_loss: 0.8110 - val_accuracy: 0.8056 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.7819 - accuracy: 0.8138 - val_loss: 0.6583 - val_accuracy: 0.8559 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.7276 - accuracy: 0.8398 - val_loss: 0.7029 - val_accuracy: 0.8594 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.6864 - accuracy: 0.8498 - val_loss: 0.6347 - val_accuracy: 0.8681 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.6156 - accuracy: 0.8746 - val_loss: 0.7391 - val_accuracy: 0.8490 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.6294 - accuracy: 0.8733 - val_loss: 0.5909 - val_accuracy: 0.8837 - lr: 5.0000e-04\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.5733 - accuracy: 0.8893 - val_loss: 0.5779 - val_accuracy: 0.8958 - lr: 5.0000e-04\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.5898 - accuracy: 0.8837 - val_loss: 0.5716 - val_accuracy: 0.8802 - lr: 5.0000e-04\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.5310 - accuracy: 0.8997 - val_loss: 0.5732 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.4982 - accuracy: 0.9093 - val_loss: 0.5566 - val_accuracy: 0.9132 - lr: 5.0000e-04\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.5024 - accuracy: 0.9049 - val_loss: 0.5953 - val_accuracy: 0.8976 - lr: 5.0000e-04\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.4848 - accuracy: 0.9141 - val_loss: 0.5393 - val_accuracy: 0.8958 - lr: 5.0000e-04\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 6s 57ms/step - loss: 0.4615 - accuracy: 0.9110 - val_loss: 0.5317 - val_accuracy: 0.8993 - lr: 5.0000e-04\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.4426 - accuracy: 0.9253 - val_loss: 0.5549 - val_accuracy: 0.9045 - lr: 5.0000e-04\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 6s 59ms/step - loss: 0.4235 - accuracy: 0.9293 - val_loss: 0.4329 - val_accuracy: 0.9253 - lr: 5.0000e-04\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.3793 - accuracy: 0.9410 - val_loss: 0.5219 - val_accuracy: 0.9219 - lr: 5.0000e-04\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.4437 - accuracy: 0.9245 - val_loss: 0.4890 - val_accuracy: 0.9045 - lr: 5.0000e-04\n",
      "Epoch 33/40\n",
      "96/96 [==============================] - 6s 57ms/step - loss: 0.4122 - accuracy: 0.9253 - val_loss: 0.4878 - val_accuracy: 0.9253 - lr: 5.0000e-04\n",
      "Epoch 34/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4031 - accuracy: 0.9336\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.4031 - accuracy: 0.9336 - val_loss: 0.4706 - val_accuracy: 0.9184 - lr: 5.0000e-04\n",
      "Epoch 35/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.3202 - accuracy: 0.9514 - val_loss: 0.4347 - val_accuracy: 0.9392 - lr: 2.5000e-04\n",
      "Epoch 36/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.3000 - accuracy: 0.9609 - val_loss: 0.4164 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 37/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.2538 - accuracy: 0.9761 - val_loss: 0.4979 - val_accuracy: 0.9236 - lr: 2.5000e-04\n",
      "Epoch 38/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.2354 - accuracy: 0.9774 - val_loss: 0.4310 - val_accuracy: 0.9410 - lr: 2.5000e-04\n",
      "Epoch 39/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.2501 - accuracy: 0.9753 - val_loss: 0.4568 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 40/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9809\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.2205 - accuracy: 0.9809 - val_loss: 0.4210 - val_accuracy: 0.9392 - lr: 2.5000e-04\n",
      "\n",
      "Phase 2: Fine-tuning autoencoder blocks...\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 15s 79ms/step - loss: 0.2259 - accuracy: 0.9800 - val_loss: 0.4207 - val_accuracy: 0.9462\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.2059 - accuracy: 0.9831 - val_loss: 0.4106 - val_accuracy: 0.9462\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1891 - accuracy: 0.9878 - val_loss: 0.4023 - val_accuracy: 0.9497\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1900 - accuracy: 0.9878 - val_loss: 0.4162 - val_accuracy: 0.9497\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 6s 57ms/step - loss: 0.1733 - accuracy: 0.9913 - val_loss: 0.4035 - val_accuracy: 0.9497\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.1824 - accuracy: 0.9874 - val_loss: 0.3848 - val_accuracy: 0.9462\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 6s 57ms/step - loss: 0.1673 - accuracy: 0.9922 - val_loss: 0.3963 - val_accuracy: 0.9531\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 6s 57ms/step - loss: 0.1586 - accuracy: 0.9922 - val_loss: 0.4032 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1606 - accuracy: 0.9896 - val_loss: 0.3859 - val_accuracy: 0.9531\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.1643 - accuracy: 0.9913 - val_loss: 0.3917 - val_accuracy: 0.9531\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1519 - accuracy: 0.9931 - val_loss: 0.4157 - val_accuracy: 0.9497\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1473 - accuracy: 0.9926 - val_loss: 0.3993 - val_accuracy: 0.9531\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9909Restoring model weights from the end of the best epoch: 7.\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 0.1570 - accuracy: 0.9909 - val_loss: 0.3946 - val_accuracy: 0.9531\n",
      "Epoch 13: early stopping\n",
      "\n",
      "Fold 3 Results:\n",
      "  Accuracy:  0.9531\n",
      "  Precision: 0.9531\n",
      "  Recall:    0.9494\n",
      "  F1-Score:  0.9509\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "Classifier parameters: 2,766,713\n",
      "\n",
      "Phase 1: Training classifier (encoder frozen)...\n",
      "Epoch 1/40\n",
      "96/96 [==============================] - 14s 72ms/step - loss: 2.9482 - accuracy: 0.1793 - val_loss: 2.3301 - val_accuracy: 0.2014 - lr: 5.0000e-04\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 2.4575 - accuracy: 0.2826 - val_loss: 2.1323 - val_accuracy: 0.2378 - lr: 5.0000e-04\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 2.1886 - accuracy: 0.3303 - val_loss: 1.7325 - val_accuracy: 0.4948 - lr: 5.0000e-04\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.9816 - accuracy: 0.4049 - val_loss: 1.6171 - val_accuracy: 0.5382 - lr: 5.0000e-04\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.8155 - accuracy: 0.4557 - val_loss: 1.4850 - val_accuracy: 0.5399 - lr: 5.0000e-04\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.6760 - accuracy: 0.4939 - val_loss: 1.3060 - val_accuracy: 0.6198 - lr: 5.0000e-04\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.5687 - accuracy: 0.5282 - val_loss: 1.3802 - val_accuracy: 0.5972 - lr: 5.0000e-04\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.4069 - accuracy: 0.5977 - val_loss: 1.3053 - val_accuracy: 0.6788 - lr: 5.0000e-04\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.3134 - accuracy: 0.6302 - val_loss: 1.1443 - val_accuracy: 0.6927 - lr: 5.0000e-04\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 6s 58ms/step - loss: 1.2108 - accuracy: 0.6654 - val_loss: 1.0341 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.0864 - accuracy: 0.7114 - val_loss: 1.2050 - val_accuracy: 0.7014 - lr: 5.0000e-04\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 1.0194 - accuracy: 0.7279 - val_loss: 1.1091 - val_accuracy: 0.7205 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.9754 - accuracy: 0.7561 - val_loss: 0.8290 - val_accuracy: 0.8003 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.8991 - accuracy: 0.7895 - val_loss: 0.8683 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.9138 - accuracy: 0.7821 - val_loss: 0.7560 - val_accuracy: 0.8299 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.7563 - accuracy: 0.8303 - val_loss: 0.6686 - val_accuracy: 0.8559 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.7396 - accuracy: 0.8372 - val_loss: 0.7134 - val_accuracy: 0.8715 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.7012 - accuracy: 0.8451 - val_loss: 0.6221 - val_accuracy: 0.8698 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.6844 - accuracy: 0.8485 - val_loss: 0.6391 - val_accuracy: 0.8750 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.6336 - accuracy: 0.8676 - val_loss: 0.5820 - val_accuracy: 0.8785 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.6139 - accuracy: 0.8767 - val_loss: 0.5701 - val_accuracy: 0.8906 - lr: 5.0000e-04\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.5905 - accuracy: 0.8793 - val_loss: 0.5004 - val_accuracy: 0.9184 - lr: 5.0000e-04\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.5624 - accuracy: 0.8945 - val_loss: 0.5186 - val_accuracy: 0.9288 - lr: 5.0000e-04\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.5390 - accuracy: 0.8984 - val_loss: 0.5436 - val_accuracy: 0.9115 - lr: 5.0000e-04\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.5096 - accuracy: 0.9149 - val_loss: 0.5561 - val_accuracy: 0.9115 - lr: 5.0000e-04\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.9188\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.4675 - accuracy: 0.9188 - val_loss: 0.5121 - val_accuracy: 0.9201 - lr: 5.0000e-04\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.4101 - accuracy: 0.9332 - val_loss: 0.4499 - val_accuracy: 0.9497 - lr: 2.5000e-04\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.3474 - accuracy: 0.9549 - val_loss: 0.4709 - val_accuracy: 0.9288 - lr: 2.5000e-04\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.3655 - accuracy: 0.9501 - val_loss: 0.3806 - val_accuracy: 0.9427 - lr: 2.5000e-04\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.3744 - accuracy: 0.9470 - val_loss: 0.4084 - val_accuracy: 0.9427 - lr: 2.5000e-04\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.3441 - accuracy: 0.9536 - val_loss: 0.3867 - val_accuracy: 0.9479 - lr: 2.5000e-04\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.3344 - accuracy: 0.9592 - val_loss: 0.4820 - val_accuracy: 0.9427 - lr: 2.5000e-04\n",
      "Epoch 33/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9683\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.3165 - accuracy: 0.9683 - val_loss: 0.4072 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 34/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2815 - accuracy: 0.9727 - val_loss: 0.4106 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 35/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2962 - accuracy: 0.9666 - val_loss: 0.3511 - val_accuracy: 0.9531 - lr: 1.2500e-04\n",
      "Epoch 36/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2731 - accuracy: 0.9740 - val_loss: 0.4075 - val_accuracy: 0.9514 - lr: 1.2500e-04\n",
      "Epoch 37/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2912 - accuracy: 0.9696 - val_loss: 0.4160 - val_accuracy: 0.9479 - lr: 1.2500e-04\n",
      "Epoch 38/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2747 - accuracy: 0.9770 - val_loss: 0.3613 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 39/40\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9803\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2558 - accuracy: 0.9805 - val_loss: 0.3876 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 40/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2422 - accuracy: 0.9822 - val_loss: 0.3816 - val_accuracy: 0.9583 - lr: 6.2500e-05\n",
      "\n",
      "Phase 2: Fine-tuning autoencoder blocks...\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 15s 78ms/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.3931 - val_accuracy: 0.9514\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.2228 - accuracy: 0.9870 - val_loss: 0.3834 - val_accuracy: 0.9479\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2289 - accuracy: 0.9844 - val_loss: 0.3832 - val_accuracy: 0.9514\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2214 - accuracy: 0.9852 - val_loss: 0.3898 - val_accuracy: 0.9514\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2207 - accuracy: 0.9835 - val_loss: 0.4146 - val_accuracy: 0.9444\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2104 - accuracy: 0.9848 - val_loss: 0.3734 - val_accuracy: 0.9549\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.2024 - accuracy: 0.9865 - val_loss: 0.3847 - val_accuracy: 0.9514\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2015 - accuracy: 0.9844 - val_loss: 0.3740 - val_accuracy: 0.9549\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1860 - accuracy: 0.9913 - val_loss: 0.3844 - val_accuracy: 0.9479\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.1735 - accuracy: 0.9922 - val_loss: 0.3912 - val_accuracy: 0.9479\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1858 - accuracy: 0.9887 - val_loss: 0.3776 - val_accuracy: 0.9514\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9900Restoring model weights from the end of the best epoch: 6.\n",
      "96/96 [==============================] - 5s 57ms/step - loss: 0.1788 - accuracy: 0.9900 - val_loss: 0.3872 - val_accuracy: 0.9444\n",
      "Epoch 12: early stopping\n",
      "\n",
      "Fold 4 Results:\n",
      "  Accuracy:  0.9549\n",
      "  Precision: 0.9570\n",
      "  Recall:    0.9478\n",
      "  F1-Score:  0.9508\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "Classifier parameters: 2,766,713\n",
      "\n",
      "Phase 1: Training classifier (encoder frozen)...\n",
      "Epoch 1/40\n",
      "96/96 [==============================] - 14s 70ms/step - loss: 2.9167 - accuracy: 0.1688 - val_loss: 2.3874 - val_accuracy: 0.1493 - lr: 5.0000e-04\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 2.3884 - accuracy: 0.2517 - val_loss: 2.1438 - val_accuracy: 0.2847 - lr: 5.0000e-04\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 2.1371 - accuracy: 0.3329 - val_loss: 1.7832 - val_accuracy: 0.4566 - lr: 5.0000e-04\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 1.9322 - accuracy: 0.3980 - val_loss: 1.6905 - val_accuracy: 0.4861 - lr: 5.0000e-04\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.7874 - accuracy: 0.4345 - val_loss: 1.5057 - val_accuracy: 0.5694 - lr: 5.0000e-04\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.6475 - accuracy: 0.4874 - val_loss: 1.3538 - val_accuracy: 0.5851 - lr: 5.0000e-04\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.5144 - accuracy: 0.5530 - val_loss: 1.3876 - val_accuracy: 0.5990 - lr: 5.0000e-04\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.4247 - accuracy: 0.5859 - val_loss: 1.2509 - val_accuracy: 0.6510 - lr: 5.0000e-04\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 1.3020 - accuracy: 0.6137 - val_loss: 1.2643 - val_accuracy: 0.6354 - lr: 5.0000e-04\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 1.2151 - accuracy: 0.6693 - val_loss: 1.1368 - val_accuracy: 0.6944 - lr: 5.0000e-04\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 1.1673 - accuracy: 0.6749 - val_loss: 0.9891 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 1.0985 - accuracy: 0.7075 - val_loss: 0.9151 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.9907 - accuracy: 0.7500 - val_loss: 0.8608 - val_accuracy: 0.8038 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.9536 - accuracy: 0.7591 - val_loss: 0.7579 - val_accuracy: 0.8385 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.8542 - accuracy: 0.7934 - val_loss: 0.7198 - val_accuracy: 0.8559 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.8152 - accuracy: 0.8099 - val_loss: 0.8065 - val_accuracy: 0.8229 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.8192 - accuracy: 0.8073 - val_loss: 0.6579 - val_accuracy: 0.8594 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.7415 - accuracy: 0.8390 - val_loss: 0.6291 - val_accuracy: 0.8681 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.6869 - accuracy: 0.8594 - val_loss: 0.6215 - val_accuracy: 0.8767 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.6812 - accuracy: 0.8516 - val_loss: 0.5313 - val_accuracy: 0.8837 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.6293 - accuracy: 0.8681 - val_loss: 0.5335 - val_accuracy: 0.9010 - lr: 5.0000e-04\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.6311 - accuracy: 0.8685 - val_loss: 0.7098 - val_accuracy: 0.8611 - lr: 5.0000e-04\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.5611 - accuracy: 0.8867 - val_loss: 0.4378 - val_accuracy: 0.9236 - lr: 5.0000e-04\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.5141 - accuracy: 0.8997 - val_loss: 0.4710 - val_accuracy: 0.9184 - lr: 5.0000e-04\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.5100 - accuracy: 0.9071 - val_loss: 0.4981 - val_accuracy: 0.9132 - lr: 5.0000e-04\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.5266 - accuracy: 0.8993 - val_loss: 0.5737 - val_accuracy: 0.8941 - lr: 5.0000e-04\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 5s 56ms/step - loss: 0.4738 - accuracy: 0.9071 - val_loss: 0.3859 - val_accuracy: 0.9392 - lr: 5.0000e-04\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.4606 - accuracy: 0.9227 - val_loss: 0.4876 - val_accuracy: 0.9201 - lr: 5.0000e-04\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.4366 - accuracy: 0.9197 - val_loss: 0.4481 - val_accuracy: 0.9306 - lr: 5.0000e-04\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.4194 - accuracy: 0.9301 - val_loss: 0.4487 - val_accuracy: 0.9288 - lr: 5.0000e-04\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3714 - accuracy: 0.9332\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.3714 - accuracy: 0.9332 - val_loss: 0.5128 - val_accuracy: 0.8993 - lr: 5.0000e-04\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.3388 - accuracy: 0.9514 - val_loss: 0.3825 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 33/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2984 - accuracy: 0.9601 - val_loss: 0.3839 - val_accuracy: 0.9444 - lr: 2.5000e-04\n",
      "Epoch 34/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2892 - accuracy: 0.9640 - val_loss: 0.3194 - val_accuracy: 0.9618 - lr: 2.5000e-04\n",
      "Epoch 35/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2912 - accuracy: 0.9657 - val_loss: 0.4404 - val_accuracy: 0.9375 - lr: 2.5000e-04\n",
      "Epoch 36/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2995 - accuracy: 0.9614 - val_loss: 0.3690 - val_accuracy: 0.9479 - lr: 2.5000e-04\n",
      "Epoch 37/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2796 - accuracy: 0.9648 - val_loss: 0.3402 - val_accuracy: 0.9531 - lr: 2.5000e-04\n",
      "Epoch 38/40\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.2790 - accuracy: 0.9670 - val_loss: 0.3127 - val_accuracy: 0.9583 - lr: 2.5000e-04\n",
      "Epoch 39/40\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.2072 - accuracy: 0.9835 - val_loss: 0.3495 - val_accuracy: 0.9497 - lr: 2.5000e-04\n",
      "Epoch 40/40\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.2333 - accuracy: 0.9727 - val_loss: 0.3360 - val_accuracy: 0.9497 - lr: 2.5000e-04\n",
      "\n",
      "Phase 2: Fine-tuning autoencoder blocks...\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 15s 76ms/step - loss: 0.2284 - accuracy: 0.9761 - val_loss: 0.3068 - val_accuracy: 0.9566\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.1927 - accuracy: 0.9870 - val_loss: 0.3011 - val_accuracy: 0.9566\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1838 - accuracy: 0.9896 - val_loss: 0.3095 - val_accuracy: 0.9514\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1810 - accuracy: 0.9887 - val_loss: 0.2754 - val_accuracy: 0.9601\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.1919 - accuracy: 0.9839 - val_loss: 0.2856 - val_accuracy: 0.9601\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.1760 - accuracy: 0.9870 - val_loss: 0.2758 - val_accuracy: 0.9566\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1797 - accuracy: 0.9874 - val_loss: 0.2700 - val_accuracy: 0.9670\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.1693 - accuracy: 0.9874 - val_loss: 0.2714 - val_accuracy: 0.9566\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1590 - accuracy: 0.9918 - val_loss: 0.2717 - val_accuracy: 0.9601\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 0.1460 - accuracy: 0.9948 - val_loss: 0.2829 - val_accuracy: 0.9601\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1541 - accuracy: 0.9905 - val_loss: 0.2993 - val_accuracy: 0.9531\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1550 - accuracy: 0.9900 - val_loss: 0.3008 - val_accuracy: 0.9635\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9948Restoring model weights from the end of the best epoch: 7.\n",
      "96/96 [==============================] - 5s 55ms/step - loss: 0.1452 - accuracy: 0.9948 - val_loss: 0.3059 - val_accuracy: 0.9566\n",
      "Epoch 13: early stopping\n",
      "\n",
      "Fold 5 Results:\n",
      "  Accuracy:  0.9670\n",
      "  Precision: 0.9661\n",
      "  Recall:    0.9658\n",
      "  F1-Score:  0.9657\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE RESULTS WITH COMBINED FEATURES (SIMPLIFIED & FIXED)\n",
      "============================================================\n",
      "\n",
      "TABLE 1: Fold-wise Performance Metrics\n",
      "--------------------------------------------------------------------------------\n",
      " fold  accuracy  precision   recall       f1  train_samples  val_samples\n",
      "    1  0.949653   0.951334 0.946488 0.948042           2304          576\n",
      "    2  0.947917   0.945220 0.951128 0.947012           2304          576\n",
      "    3  0.953125   0.953095 0.949419 0.950916           2304          576\n",
      "    4  0.954861   0.957030 0.947753 0.950837           2304          576\n",
      "    5  0.967014   0.966067 0.965781 0.965710           2304          576\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TABLE 4: Overall Model Performance Summary\n",
      "--------------------------------------------------------------------------------\n",
      "   Metric   Mean Std Dev    Min    Max\n",
      " Accuracy 0.9545 ±0.0067 0.9479 0.9670\n",
      "Precision 0.9545 ±0.0069 0.9452 0.9661\n",
      "   Recall 0.9521 ±0.0070 0.9465 0.9658\n",
      " F1-Score 0.9525 ±0.0068 0.9470 0.9657\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE - SIMPLIFIED & FIXED MODEL\n",
      "============================================================\n",
      "\n",
      "Key Results:\n",
      "Overall Accuracy: 95.45% ± 0.67%\n",
      "Overall F1-Score: 95.25% ± 0.68%\n",
      "\n",
      "Model Architecture:\n",
      "• Multi-scale CNN autoencoder with channel attention\n",
      "• Residual connections in classifier\n",
      "• Bidirectional LSTM with attention mechanism\n",
      "• Early stopping and learning rate scheduling\n",
      "• Optimized hyperparameters for emotion recognition\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Complete SER Pipeline with Specific + Traditional Features (Fixed & Optimized)\n",
    "import librosa\n",
    "import librosa.feature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, signal\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, confusion_matrix, classification_report)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration - Optimized parameters\n",
    "SR = 22050\n",
    "DURATION = 3.0\n",
    "OFFSET = 0.5\n",
    "MAX_TIMESTEPS = 130\n",
    "\n",
    "# OPTIMIZED HYPERPARAMETERS\n",
    "AUTOENCODER_EPOCHS = 50\n",
    "CLASSIFIER_EPOCHS = 40\n",
    "FINE_TUNE_EPOCHS = 20\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "# Combined feature extraction functions (Specific + Traditional)\n",
    "def extract_frame_features(audio, sr=SR):\n",
    "    \"\"\"Extract frame-level features including both specific and traditional features\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    # ==================== TRADITIONAL FEATURES ====================\n",
    "    # 1. MFCCs (40 features)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512)\n",
    "    features_list.append(mfcc)\n",
    "    \n",
    "    # 2. Delta MFCCs (20 features)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc, width=3, order=1)[:20]\n",
    "    features_list.append(delta_mfcc)\n",
    "    \n",
    "    # 3. Delta2 MFCCs (20 features)\n",
    "    delta2_mfcc = librosa.feature.delta(mfcc, width=3, order=2)[:20]\n",
    "    features_list.append(delta2_mfcc)\n",
    "    \n",
    "    # 4. Log-Mel Spectrogram (40 features)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=40,\n",
    "                                              n_fft=2048, hop_length=512)\n",
    "    log_mel = librosa.power_to_db(mel_spec)\n",
    "    features_list.append(log_mel)\n",
    "    \n",
    "    # ==================== SPECIFIC FEATURES ====================\n",
    "    # 5. Spectral Contrast (7 features)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, n_bands=6)\n",
    "    features_list.append(spectral_contrast)\n",
    "    \n",
    "    # 6. Zero Crossing Rate (1 feature)\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio, frame_length=2048, hop_length=512)\n",
    "    features_list.append(zcr)\n",
    "    \n",
    "    # 7. Spectral Flux (1 feature)\n",
    "    spectral_flux = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "    spectral_flux = spectral_flux.reshape(1, -1)\n",
    "    features_list.append(spectral_flux)\n",
    "    \n",
    "    # Stack all frame features\n",
    "    frame_features = np.vstack(features_list)\n",
    "    return frame_features.T\n",
    "\n",
    "def extract_utterance_features(audio, sr=SR):\n",
    "    \"\"\"Extract utterance-level features including both specific and traditional features\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # ==================== SPECIFIC FEATURES ====================\n",
    "    # 1. Spectral Entropy\n",
    "    spectral_power = np.abs(librosa.stft(audio, n_fft=2048, hop_length=512))**2\n",
    "    spectral_sum = np.sum(spectral_power, axis=1)\n",
    "    spectral_probs = spectral_sum / (np.sum(spectral_sum) + 1e-10)\n",
    "    spectral_entropy = -np.sum(spectral_probs * np.log(spectral_probs + 1e-10))\n",
    "    features.append(spectral_entropy)\n",
    "    \n",
    "    # 2. Renyi Entropy (alpha=2)\n",
    "    spectral_probs_renyi = spectral_probs**2\n",
    "    renyi_entropy = -np.log(np.sum(spectral_probs_renyi) + 1e-10)\n",
    "    features.append(renyi_entropy)\n",
    "    \n",
    "    # 3. Teager Energy Operator (TEO)\n",
    "    teo = audio[1:-1]**2 - audio[:-2] * audio[2:]\n",
    "    teo_mean = np.mean(teo)\n",
    "    teo_std = np.std(teo)\n",
    "    teo_entropy = stats.entropy(np.histogram(teo, bins=20)[0] + 1e-10)\n",
    "    features.extend([teo_mean, teo_std, teo_entropy])\n",
    "    \n",
    "    # 4. HPSS (Harmonic-Percussive Source Separation) features\n",
    "    harmonic, percussive = librosa.effects.hpss(audio)\n",
    "    \n",
    "    # Harmonic-to-percussive ratio\n",
    "    harmonic_energy = np.sum(harmonic**2)\n",
    "    percussive_energy = np.sum(percussive**2)\n",
    "    hpr = harmonic_energy / (percussive_energy + 1e-10)\n",
    "    features.append(hpr)\n",
    "    \n",
    "    # Harmonic and percussive RMS\n",
    "    harmonic_rms = np.sqrt(np.mean(harmonic**2))\n",
    "    percussive_rms = np.sqrt(np.mean(percussive**2))\n",
    "    features.extend([harmonic_rms, percussive_rms])\n",
    "    \n",
    "    # 5. Enhanced ZCR features\n",
    "    zcr_frame = librosa.feature.zero_crossing_rate(audio, frame_length=2048, hop_length=512)[0]\n",
    "    zcr_mean = np.mean(zcr_frame)\n",
    "    zcr_std = np.std(zcr_frame)\n",
    "    zcr_entropy = stats.entropy(np.histogram(zcr_frame, bins=20)[0] + 1e-10)\n",
    "    features.extend([zcr_mean, zcr_std, zcr_entropy])\n",
    "    \n",
    "    # 6. Spectral Contrast statistics\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, n_bands=6)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "    spectral_contrast_std = np.std(spectral_contrast, axis=1)\n",
    "    features.extend(spectral_contrast_mean.tolist())\n",
    "    features.extend(spectral_contrast_std.tolist())\n",
    "    \n",
    "    # 7. Spectral Flux statistics\n",
    "    spectral_flux = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "    spectral_flux_mean = np.mean(spectral_flux)\n",
    "    spectral_flux_std = np.std(spectral_flux)\n",
    "    spectral_flux_entropy = stats.entropy(np.histogram(spectral_flux, bins=20)[0] + 1e-10)\n",
    "    features.extend([spectral_flux_mean, spectral_flux_std, spectral_flux_entropy])\n",
    "    \n",
    "    # 8. VMD-like features (simplified)\n",
    "    def get_imf_features(signal):\n",
    "        \"\"\"Extract IMF-like features\"\"\"\n",
    "        analytic_signal = hilbert(signal)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "        instantaneous_frequency = np.diff(instantaneous_phase) / (2.0 * np.pi) * sr\n",
    "        \n",
    "        env_mean = np.mean(amplitude_envelope)\n",
    "        env_std = np.std(amplitude_envelope)\n",
    "        freq_mean = np.mean(instantaneous_frequency) if len(instantaneous_frequency) > 0 else 0\n",
    "        freq_std = np.std(instantaneous_frequency) if len(instantaneous_frequency) > 0 else 0\n",
    "        \n",
    "        return [env_mean, env_std, freq_mean, freq_std]\n",
    "    \n",
    "    imf_features = get_imf_features(audio)\n",
    "    features.extend(imf_features)\n",
    "    \n",
    "    # ==================== ADDITIONAL ENERGY FEATURES ====================\n",
    "    # 9. RMS Energy and Entropy\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_entropy = stats.entropy(np.histogram(rms, bins=20)[0] + 1e-10)\n",
    "    features.extend([rms_mean, rms_std, rms_entropy])\n",
    "    \n",
    "    # 10. Total Energy\n",
    "    total_energy = np.sum(audio**2)\n",
    "    features.append(total_energy)\n",
    "    \n",
    "    # 11. Energy Entropy\n",
    "    frames = librosa.util.frame(audio, frame_length=2048, hop_length=512)\n",
    "    frame_energies = np.sum(frames**2, axis=0)\n",
    "    frame_energies_normalized = frame_energies / (np.sum(frame_energies) + 1e-10)\n",
    "    energy_entropy = -np.sum(frame_energies_normalized * np.log(frame_energies_normalized + 1e-10))\n",
    "    features.append(energy_entropy)\n",
    "    \n",
    "    # 12. Spectral Energy Entropy\n",
    "    spectral_energy = np.abs(librosa.stft(audio, n_fft=2048, hop_length=512))\n",
    "    spectral_energy_sum = np.sum(spectral_energy, axis=1)\n",
    "    spectral_energy_normalized = spectral_energy_sum / (np.sum(spectral_energy_sum) + 1e-10)\n",
    "    spectral_energy_entropy = -np.sum(spectral_energy_normalized * np.log(spectral_energy_normalized + 1e-10))\n",
    "    features.append(spectral_energy_entropy)\n",
    "    \n",
    "    # 13. Spectral Energy in Four Subbands\n",
    "    n_fft = 2048\n",
    "    freq_bins = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    stft = np.abs(librosa.stft(audio, n_fft=n_fft, hop_length=512))\n",
    "    \n",
    "    subband_limits = [0, 500, 1000, 2000, sr/2]\n",
    "    for i in range(len(subband_limits)-1):\n",
    "        mask = (freq_bins >= subband_limits[i]) & (freq_bins < subband_limits[i+1])\n",
    "        subband_energy = np.sum(stft[mask, :])\n",
    "        features.append(subband_energy)\n",
    "    \n",
    "    # 14. Permutation Entropy\n",
    "    def permutation_entropy(signal_data, m=3, delay=1):\n",
    "        n = len(signal_data)\n",
    "        permutations = []\n",
    "        for i in range(n - (m-1)*delay):\n",
    "            segment = signal_data[i:i + m*delay:delay]\n",
    "            permutations.append(tuple(np.argsort(segment)))\n",
    "        unique_perms, counts = np.unique(permutations, return_counts=True)\n",
    "        probs = counts / np.sum(counts)\n",
    "        return -np.sum(probs * np.log(probs + 1e-10))\n",
    "    \n",
    "    perm_entropy = permutation_entropy(audio[:min(len(audio), 1000)], m=3, delay=1)\n",
    "    features.append(perm_entropy)\n",
    "    \n",
    "    # 15. Skewness and Kurtosis\n",
    "    skewness = stats.skew(audio)\n",
    "    kurtosis = stats.kurtosis(audio)\n",
    "    features.extend([skewness, kurtosis])\n",
    "    \n",
    "    # 16. Spectral Roll-off statistics\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, roll_percent=0.85)[0]\n",
    "    features.extend([np.mean(spectral_rolloff), np.std(spectral_rolloff)])\n",
    "    \n",
    "    # 17. Pitch features\n",
    "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
    "    pitch_values = pitches[magnitudes > np.max(magnitudes) * 0.1]\n",
    "    if len(pitch_values) > 0:\n",
    "        pitch_mean = np.mean(pitch_values)\n",
    "        pitch_std = np.std(pitch_values)\n",
    "        pitch_entropy = stats.entropy(np.histogram(pitch_values, bins=20)[0] + 1e-10)\n",
    "    else:\n",
    "        pitch_mean = pitch_std = pitch_entropy = 0\n",
    "    features.extend([pitch_mean, pitch_std, pitch_entropy])\n",
    "    \n",
    "    # 18. Chroma features statistics\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    chroma_std = np.std(chroma, axis=1)\n",
    "    features.extend(chroma_mean.tolist())\n",
    "    features.extend(chroma_std.tolist())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def process_audio(filepath):\n",
    "    \"\"\"Process single audio file with combined features\"\"\"\n",
    "    audio, sr = librosa.load(filepath, sr=SR, duration=DURATION, offset=OFFSET)\n",
    "    \n",
    "    # Frame-level features (traditional + specific)\n",
    "    frame_feats = extract_frame_features(audio, sr)\n",
    "    \n",
    "    # Utterance-level features (specific + additional)\n",
    "    utterance_feats = extract_utterance_features(audio, sr)\n",
    "    utterance_feats = utterance_feats.reshape(1, -1)\n",
    "    \n",
    "    # Repeat utterance features across time\n",
    "    utterance_feats_rep = np.repeat(utterance_feats, frame_feats.shape[0], axis=0)\n",
    "    \n",
    "    # Combine features\n",
    "    features = np.hstack([frame_feats, utterance_feats_rep])\n",
    "    \n",
    "    # Pad/truncate to fixed timesteps\n",
    "    if features.shape[0] < MAX_TIMESTEPS:\n",
    "        pad_width = ((0, MAX_TIMESTEPS - features.shape[0]), (0, 0))\n",
    "        features = np.pad(features, pad_width, mode='constant')\n",
    "    else:\n",
    "        features = features[:MAX_TIMESTEPS, :]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Process all files\n",
    "print(\"Extracting combined features (Specific + Traditional)...\")\n",
    "X = []\n",
    "for i, fp in enumerate(filepaths):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i}/{len(filepaths)} files...\")\n",
    "    X.append(process_audio(fp))\n",
    "X = np.array(X)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_onehot = tf.keras.utils.to_categorical(y_encoded)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(f\"\\nCombined feature extraction complete:\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y_onehot.shape}\")\n",
    "print(f\"Number of features per timestep: {X.shape[2]}\")\n",
    "\n",
    "# Calculate feature breakdown\n",
    "temp_audio = np.zeros(22050)  # 1 second of silence for testing\n",
    "frame_features_count = extract_frame_features(temp_audio).shape[1]\n",
    "utterance_features_count = extract_utterance_features(temp_audio).shape[0]\n",
    "\n",
    "print(f\"\\nDetailed Feature Breakdown:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"FRAME-LEVEL FEATURES ({frame_features_count} features):\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Traditional Features:\")\n",
    "print(f\"  • MFCCs: 40 features\")\n",
    "print(f\"  • Delta MFCCs: 20 features\")\n",
    "print(f\"  • Delta2 MFCCs: 20 features\")\n",
    "print(f\"  • Log-Mel Spectrogram: 40 features\")\n",
    "print(f\"\\nSpecific Features:\")\n",
    "print(f\"  • Spectral Contrast: 7 features (6 bands + mean)\")\n",
    "print(f\"  • Zero Crossing Rate: 1 feature\")\n",
    "print(f\"  • Spectral Flux: 1 feature\")\n",
    "print(f\"  Frame-level Total: {frame_features_count} features\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"UTTERANCE-LEVEL FEATURES ({utterance_features_count} features):\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Specific Features:\")\n",
    "print(f\"  • Spectral Entropy: 1 feature\")\n",
    "print(f\"  • Renyi Entropy: 1 feature\")\n",
    "print(f\"  • TEO features: 3 features\")\n",
    "print(f\"  • HPSS features: 3 features\")\n",
    "print(f\"  • Enhanced ZCR: 3 features\")\n",
    "print(f\"  • Spectral Contrast stats: 14 features\")\n",
    "print(f\"  • Spectral Flux stats: 3 features\")\n",
    "print(f\"  • VMD-like features: 4 features\")\n",
    "print(f\"  • RMS Energy & Entropy: 3 features\")\n",
    "print(f\"  • Total Energy: 1 feature\")\n",
    "print(f\"  • Energy Entropy: 1 feature\")\n",
    "print(f\"  • Spectral Energy Entropy: 1 feature\")\n",
    "print(f\"  • Spectral Energy in 4 Subbands: 4 features\")\n",
    "print(f\"  • Permutation Entropy: 1 feature\")\n",
    "print(f\"  • Skewness & Kurtosis: 2 features\")\n",
    "print(f\"  • Spectral Roll-off stats: 2 features\")\n",
    "print(f\"  • Pitch features: 3 features\")\n",
    "print(f\"  • Chroma features stats: 24 features\")\n",
    "print(f\"  Utterance-level Total: {utterance_features_count} features\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"GRAND TOTAL: {frame_features_count + utterance_features_count} FEATURES\")\n",
    "print(f\"  • Frame-level: {frame_features_count} features\")\n",
    "print(f\"  • Utterance-level: {utterance_features_count} features\")\n",
    "print(f\"  • Combined: {X.shape[2]} features per timestep\")\n",
    "\n",
    "# Scale features globally\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "X_scaled = scaler.fit_transform(X_reshaped).reshape(X.shape)\n",
    "\n",
    "# FIXED & SIMPLIFIED Model Definition\n",
    "def create_autoencoder(input_shape, name_suffix=\"\"):\n",
    "    \"\"\"Simplified and Fixed Multi-scale 1D CNN Autoencoder\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name=f'input_{name_suffix}')\n",
    "    orig_timesteps = input_shape[0]\n",
    "    \n",
    "    # Initial convolution\n",
    "    x = layers.Conv1D(64, 7, padding='same', activation='relu',\n",
    "                     name=f'initial_conv_{name_suffix}')(inputs)\n",
    "    x = layers.BatchNormalization(name=f'initial_bn_{name_suffix}')(x)\n",
    "    \n",
    "    # Multi-scale feature extraction\n",
    "    conv_outs = []\n",
    "    for i, kernel in enumerate([3, 5, 7]):\n",
    "        conv = layers.Conv1D(64, kernel, padding='same', activation='relu',\n",
    "                           name=f'conv1_{kernel}_{name_suffix}')(x)\n",
    "        conv = layers.BatchNormalization(name=f'bn1_{kernel}_{name_suffix}')(conv)\n",
    "        conv = layers.Conv1D(64, kernel, padding='same', activation='relu',\n",
    "                           name=f'conv2_{kernel}_{name_suffix}')(conv)\n",
    "        conv = layers.BatchNormalization(name=f'bn2_{kernel}_{name_suffix}')(conv)\n",
    "        conv = layers.MaxPooling1D(2, name=f'pool_{kernel}_{name_suffix}')(conv)\n",
    "        conv_outs.append(conv)\n",
    "    \n",
    "    # Merge multi-scale features\n",
    "    if len(conv_outs) > 1:\n",
    "        merged = layers.Concatenate(name=f'concat_{name_suffix}')(conv_outs)\n",
    "    else:\n",
    "        merged = conv_outs[0]\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Conv1D(128, 3, padding='same', activation='relu',\n",
    "                     name=f'enc_conv1_{name_suffix}')(merged)\n",
    "    x = layers.BatchNormalization(name=f'enc_bn1_{name_suffix}')(x)\n",
    "    x = layers.MaxPooling1D(2, name=f'enc_pool1_{name_suffix}')(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                     name=f'enc_conv2_{name_suffix}')(x)\n",
    "    x = layers.BatchNormalization(name=f'enc_bn2_{name_suffix}')(x)\n",
    "    x = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                     name=f'enc_conv3_{name_suffix}')(x)\n",
    "    x = layers.BatchNormalization(name=f'enc_bn3_{name_suffix}')(x)\n",
    "    \n",
    "    # Simplified attention mechanism\n",
    "    def channel_attention(x):\n",
    "        # Squeeze: Global Average Pooling\n",
    "        se = layers.GlobalAveragePooling1D()(x)\n",
    "        # Excitation: Two FC layers with ReLU and Sigmoid\n",
    "        se = layers.Dense(256 // 16, activation='relu')(se)\n",
    "        se = layers.Dense(256, activation='sigmoid')(se)\n",
    "        # Reshape for broadcasting\n",
    "        se = layers.Reshape((1, 256))(se)\n",
    "        # Scale the input\n",
    "        return layers.Multiply()([x, se])\n",
    "    \n",
    "    x = channel_attention(x)\n",
    "    \n",
    "    bottleneck = layers.MaxPooling1D(2, name=f'bottleneck_{name_suffix}')(x)\n",
    "    bottleneck = layers.Dropout(0.3, name=f'bottleneck_dropout_{name_suffix}')(bottleneck)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                     name=f'dec_conv1_{name_suffix}')(bottleneck)\n",
    "    x = layers.BatchNormalization(name=f'dec_bn1_{name_suffix}')(x)\n",
    "    x = layers.UpSampling1D(2, name=f'upsample1_{name_suffix}')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 3, padding='same', activation='relu',\n",
    "                     name=f'dec_conv2_{name_suffix}')(x)\n",
    "    x = layers.BatchNormalization(name=f'dec_bn2_{name_suffix}')(x)\n",
    "    x = layers.UpSampling1D(2, name=f'upsample2_{name_suffix}')(x)\n",
    "    \n",
    "    # Multi-scale reconstruction\n",
    "    recon_outs = []\n",
    "    for i, kernel in enumerate([3, 5, 7]):\n",
    "        rec = layers.Conv1D(64, kernel, padding='same', activation='relu',\n",
    "                           name=f'deconv_{kernel}_{name_suffix}')(x)\n",
    "        rec = layers.BatchNormalization(name=f'deconv_bn_{kernel}_{name_suffix}')(rec)\n",
    "        rec = layers.UpSampling1D(2, name=f'upsample3_{kernel}_{name_suffix}')(rec)\n",
    "        \n",
    "        # Adjust to match original timesteps\n",
    "        if rec.shape[1] < orig_timesteps:\n",
    "            padding = orig_timesteps - rec.shape[1]\n",
    "            rec = layers.ZeroPadding1D((0, padding), name=f'pad_{kernel}_{name_suffix}')(rec)\n",
    "        elif rec.shape[1] > orig_timesteps:\n",
    "            cropping = rec.shape[1] - orig_timesteps\n",
    "            rec = layers.Cropping1D((0, cropping), name=f'crop_{kernel}_{name_suffix}')(rec)\n",
    "        recon_outs.append(rec)\n",
    "    \n",
    "    if len(recon_outs) > 1:\n",
    "        merged_rec = layers.Average(name=f'avg_recon_{name_suffix}')(recon_outs)\n",
    "    else:\n",
    "        merged_rec = recon_outs[0]\n",
    "    \n",
    "    # Final convolution\n",
    "    outputs = layers.Conv1D(input_shape[1], 3, padding='same', activation='linear',\n",
    "                           name=f'output_{name_suffix}')(merged_rec)\n",
    "    \n",
    "    return Model(inputs, outputs, name=f'autoencoder_{name_suffix}'), \\\n",
    "           Model(inputs, bottleneck, name=f'encoder_{name_suffix}')\n",
    "\n",
    "def create_classifier(encoder, num_classes, fold_num):\n",
    "    \"\"\"Enhanced Classifier with attention mechanism\"\"\"\n",
    "    encoder_output = encoder.output\n",
    "    \n",
    "    # CNN layers with residual connections\n",
    "    # Block 1\n",
    "    x1 = layers.Conv1D(256, 5, padding='same', activation='relu',\n",
    "                      name=f'classifier_conv1_fold{fold_num}')(encoder_output)\n",
    "    x1 = layers.BatchNormalization(name=f'classifier_bn1_fold{fold_num}')(x1)\n",
    "    x1 = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                      name=f'classifier_conv2_fold{fold_num}')(x1)\n",
    "    x1 = layers.BatchNormalization(name=f'classifier_bn2_fold{fold_num}')(x1)\n",
    "    x1 = layers.Dropout(0.25, name=f'classifier_dropout1_fold{fold_num}')(x1)\n",
    "    \n",
    "    # Residual connection\n",
    "    shortcut = layers.Conv1D(256, 1, padding='same', \n",
    "                           name=f'shortcut1_fold{fold_num}')(encoder_output)\n",
    "    x1 = layers.Add(name=f'residual1_fold{fold_num}')([x1, shortcut])\n",
    "    x1 = layers.Activation('relu', name=f'residual1_activation_fold{fold_num}')(x1)\n",
    "    x1 = layers.MaxPooling1D(2, name=f'classifier_pool1_fold{fold_num}')(x1)\n",
    "    \n",
    "    # Block 2\n",
    "    x2 = layers.Conv1D(192, 5, padding='same', activation='relu',\n",
    "                      name=f'classifier_conv3_fold{fold_num}')(x1)\n",
    "    x2 = layers.BatchNormalization(name=f'classifier_bn3_fold{fold_num}')(x2)\n",
    "    x2 = layers.Conv1D(192, 3, padding='same', activation='relu',\n",
    "                      name=f'classifier_conv4_fold{fold_num}')(x2)\n",
    "    x2 = layers.BatchNormalization(name=f'classifier_bn4_fold{fold_num}')(x2)\n",
    "    x2 = layers.Dropout(0.25, name=f'classifier_dropout2_fold{fold_num}')(x2)\n",
    "    \n",
    "    # Residual connection\n",
    "    shortcut2 = layers.Conv1D(192, 1, padding='same',\n",
    "                            name=f'shortcut2_fold{fold_num}')(x1)\n",
    "    x2 = layers.Add(name=f'residual2_fold{fold_num}')([x2, shortcut2])\n",
    "    x2 = layers.Activation('relu', name=f'residual2_activation_fold{fold_num}')(x2)\n",
    "    x2 = layers.MaxPooling1D(2, name=f'classifier_pool2_fold{fold_num}')(x2)\n",
    "    \n",
    "    # LSTM layers\n",
    "    lstm1 = layers.Bidirectional(\n",
    "        layers.LSTM(192, return_sequences=True, dropout=0.2, recurrent_dropout=0.1,\n",
    "                   name=f'lstm1_fold{fold_num}'),\n",
    "        name=f'bidirectional1_fold{fold_num}'\n",
    "    )(x2)\n",
    "    \n",
    "    lstm2 = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.1,\n",
    "                   name=f'lstm2_fold{fold_num}'),\n",
    "        name=f'bidirectional2_fold{fold_num}'\n",
    "    )(lstm1)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = layers.Dense(1, activation='tanh',\n",
    "                            name=f'attention_dense_fold{fold_num}')(lstm2)\n",
    "    attention = layers.Flatten(name=f'attention_flatten_fold{fold_num}')(attention)\n",
    "    attention = layers.Activation('softmax', name=f'attention_softmax_fold{fold_num}')(attention)\n",
    "    attention = layers.RepeatVector(128*2, name=f'attention_repeat_fold{fold_num}')(attention)\n",
    "    attention = layers.Permute([2, 1], name=f'attention_permute_fold{fold_num}')(attention)\n",
    "    weighted = layers.Multiply(name=f'attention_multiply_fold{fold_num}')([lstm2, attention])\n",
    "    weighted = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1),\n",
    "                            name=f'attention_sum_fold{fold_num}')(weighted)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(128, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                    name=f'classifier_dense1_fold{fold_num}')(weighted)\n",
    "    x = layers.BatchNormalization(name=f'dense_bn1_fold{fold_num}')(x)\n",
    "    x = layers.Dropout(0.4, name=f'classifier_dropout3_fold{fold_num}')(x)\n",
    "    \n",
    "    x = layers.Dense(96, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                    name=f'classifier_dense2_fold{fold_num}')(x)\n",
    "    x = layers.BatchNormalization(name=f'dense_bn2_fold{fold_num}')(x)\n",
    "    x = layers.Dropout(0.4, name=f'classifier_dropout4_fold{fold_num}')(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu',\n",
    "                    name=f'classifier_dense3_fold{fold_num}')(x)\n",
    "    x = layers.BatchNormalization(name=f'dense_bn3_fold{fold_num}')(x)\n",
    "    x = layers.Dropout(0.3, name=f'classifier_dropout5_fold{fold_num}')(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                          name=f'classifier_output_fold{fold_num}')(x)\n",
    "    \n",
    "    return Model(encoder.input, outputs, name=f'classifier_fold{fold_num}')\n",
    "\n",
    "# Step 1: Train autoencoder on all data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Training Autoencoder on all data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "autoencoder, encoder = create_autoencoder((MAX_TIMESTEPS, X_scaled.shape[2]), \"base\")\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                    loss='mse',\n",
    "                    metrics=['mae'])\n",
    "\n",
    "print(f\"Autoencoder architecture summary:\")\n",
    "print(f\"Total parameters: {autoencoder.count_params():,}\")\n",
    "\n",
    "print(\"\\nTraining autoencoder with early stopping...\")\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add learning rate scheduler\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_ae = autoencoder.fit(X_scaled, X_scaled,\n",
    "                             epochs=AUTOENCODER_EPOCHS,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             validation_split=0.1,\n",
    "                             callbacks=[early_stopping, lr_scheduler],\n",
    "                             verbose=1)\n",
    "\n",
    "# Plot autoencoder training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ae.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history_ae.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Autoencoder Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_ae.history['mae'], label='Training MAE', linewidth=2)\n",
    "plt.plot(history_ae.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "plt.title('Autoencoder MAE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_training_fixed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Autoencoder final loss: {history_ae.history['loss'][-1]:.4f}\")\n",
    "print(f\"Autoencoder final val loss: {history_ae.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Step 2: 5-Fold Cross Validation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: 5-Fold Cross Validation for Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "fold_details = []\n",
    "all_confusion_matrices = []\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "fold_histories = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y_encoded), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/5\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]\n",
    "    \n",
    "    # Create fresh classifier with unique names\n",
    "    autoencoder_fold, encoder_fold = create_autoencoder((MAX_TIMESTEPS, X_scaled.shape[2]),\n",
    "                                                       f\"fold{fold}\")\n",
    "    # Copy weights from the base autoencoder\n",
    "    for i in range(len(autoencoder_fold.layers)):\n",
    "        if i < len(autoencoder.layers):\n",
    "            autoencoder_fold.layers[i].set_weights(autoencoder.layers[i].get_weights())\n",
    "    \n",
    "    # Create classifier with fold-specific names\n",
    "    classifier = create_classifier(encoder_fold, num_classes=len(class_names), fold_num=fold)\n",
    "    \n",
    "    print(f\"Classifier parameters: {classifier.count_params():,}\")\n",
    "    \n",
    "    # Phase 1: Train classifier with frozen encoder\n",
    "    encoder_fold.trainable = False\n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for phase 1\n",
    "    early_stopping1 = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lr_scheduler1 = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPhase 1: Training classifier (encoder frozen)...\")\n",
    "    history1 = classifier.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val),\n",
    "                              epochs=CLASSIFIER_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              callbacks=[early_stopping1, lr_scheduler1],\n",
    "                              verbose=1)\n",
    "    \n",
    "    # Phase 2: Fine-tune\n",
    "    encoder_fold.trainable = True\n",
    "    # Freeze all layers except the last 4\n",
    "    for i, layer in enumerate(encoder_fold.layers):\n",
    "        if i < len(encoder_fold.layers) - 4:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for phase 2\n",
    "    early_stopping2 = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=6,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPhase 2: Fine-tuning autoencoder blocks...\")\n",
    "    history2 = classifier.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val),\n",
    "                              epochs=FINE_TUNE_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              callbacks=[early_stopping2],\n",
    "                              verbose=1)\n",
    "    \n",
    "    # Store training history\n",
    "    fold_history = {\n",
    "        'fold': fold,\n",
    "        'phase1_loss': history1.history['loss'],\n",
    "        'phase1_val_loss': history1.history['val_loss'],\n",
    "        'phase1_accuracy': history1.history['accuracy'],\n",
    "        'phase1_val_accuracy': history1.history['val_accuracy'],\n",
    "        'phase2_loss': history2.history['loss'],\n",
    "        'phase2_val_loss': history2.history['val_loss'],\n",
    "        'phase2_accuracy': history2.history['accuracy'],\n",
    "        'phase2_val_accuracy': history2.history['val_accuracy']\n",
    "    }\n",
    "    fold_histories.append(fold_history)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = classifier.predict(X_val, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Store for overall evaluation\n",
    "    all_y_true.extend(y_true_classes)\n",
    "    all_y_pred.extend(y_pred_classes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    precision = precision_score(y_true_classes, y_pred_classes, average='macro')\n",
    "    recall = recall_score(y_true_classes, y_pred_classes, average='macro')\n",
    "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_report = classification_report(y_true_classes, y_pred_classes,\n",
    "                                         target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    all_confusion_matrices.append(cm)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val)\n",
    "    })\n",
    "    \n",
    "    # Store per-class details\n",
    "    for emotion in class_names:\n",
    "        if emotion in class_report:\n",
    "            fold_details.append({\n",
    "                'fold': fold,\n",
    "                'emotion': emotion,\n",
    "                'accuracy': class_report[emotion]['precision'],\n",
    "                'precision': class_report[emotion]['precision'],\n",
    "                'recall': class_report[emotion]['recall'],\n",
    "                'f1': class_report[emotion]['f1-score'],\n",
    "                'support': class_report[emotion]['support']\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Continue with the rest of the code (visualizations, tables, etc.)...\n",
    "# The rest of the visualization and reporting code remains the same as before\n",
    "\n",
    "# Calculate overall metrics\n",
    "avg_accuracy = np.mean([r['accuracy'] for r in fold_results])\n",
    "avg_precision = np.mean([r['precision'] for r in fold_results])\n",
    "avg_recall = np.mean([r['recall'] for r in fold_results])\n",
    "avg_f1 = np.mean([r['f1'] for r in fold_results])\n",
    "\n",
    "std_accuracy = np.std([r['accuracy'] for r in fold_results])\n",
    "std_precision = np.std([r['precision'] for r in fold_results])\n",
    "std_recall = np.std([r['recall'] for r in fold_results])\n",
    "std_f1 = np.std([r['f1'] for r in fold_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE RESULTS WITH COMBINED FEATURES (SIMPLIFIED & FIXED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Table 1: Overall fold-wise results\n",
    "print(\"\\nTABLE 1: Fold-wise Performance Metrics\")\n",
    "print(\"-\"*80)\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "print(fold_df.to_string(index=False))\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nTABLE 4: Overall Model Performance Summary\")\n",
    "print(\"-\"*80)\n",
    "summary_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Mean': [f\"{avg_accuracy:.4f}\", f\"{avg_precision:.4f}\",\n",
    "             f\"{avg_recall:.4f}\", f\"{avg_f1:.4f}\"],\n",
    "    'Std Dev': [f\"±{std_accuracy:.4f}\", f\"±{std_precision:.4f}\",\n",
    "                f\"±{std_recall:.4f}\", f\"±{std_f1:.4f}\"],\n",
    "    'Min': [f\"{min([r['accuracy'] for r in fold_results]):.4f}\",\n",
    "            f\"{min([r['precision'] for r in fold_results]):.4f}\",\n",
    "            f\"{min([r['recall'] for r in fold_results]):.4f}\",\n",
    "            f\"{min([r['f1'] for r in fold_results]):.4f}\"],\n",
    "    'Max': [f\"{max([r['accuracy'] for r in fold_results]):.4f}\",\n",
    "            f\"{max([r['precision'] for r in fold_results]):.4f}\",\n",
    "            f\"{max([r['recall'] for r in fold_results]):.4f}\",\n",
    "            f\"{max([r['f1'] for r in fold_results]):.4f}\"]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - SIMPLIFIED & FIXED MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey Results:\")\n",
    "print(f\"Overall Accuracy: {avg_accuracy:.2%} ± {std_accuracy:.2%}\")\n",
    "print(f\"Overall F1-Score: {avg_f1:.2%} ± {std_f1:.2%}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"• Multi-scale CNN autoencoder with channel attention\")\n",
    "print(f\"• Residual connections in classifier\")\n",
    "print(f\"• Bidirectional LSTM with attention mechanism\")\n",
    "print(f\"• Early stopping and learning rate scheduling\")\n",
    "print(f\"• Optimized hyperparameters for emotion recognition\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
